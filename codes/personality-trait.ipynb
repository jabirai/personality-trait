{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install transformers\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install accelerate -U\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Get all the transcripts in the transcript folder\n",
    "transcript_folder = './dataset/two_speakers_ID/'  \n",
    "\n",
    "# Initialize an empty list to collect DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Regex pattern to extract patient_id and provider_id from filenames\n",
    "pattern = re.compile(r'(\\d+)_(\\d+)_\\d+\\.csv')\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(transcript_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract patient_id and provider_id from filename\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            patient_id, provider_id = match.groups()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(transcript_folder, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add patient_id and provider_id as new columns\n",
    "        df['Patient_ID'] = patient_id\n",
    "        df['Provider_ID'] = provider_id\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# combined_df.to_csv('combined_transcripts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('combined_transcripts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analysis only for SVS\n",
    "transcript_df = combined_df\n",
    "\n",
    "# Load the SVS scores data\n",
    "scores_df = pd.read_csv('./dataset/survey.csv') \n",
    "\n",
    "# List of required SVS columns\n",
    "required_columns = [\n",
    "    'patient_id','Conservation', 'Conformity', 'Tradition', 'Security', \n",
    "    'Self-Transcendance', 'Benevolence', 'Universalism', \n",
    "    'Self-Enhancement', 'Power', 'Achievement', \n",
    "    'Stimulation', 'Openness to Change', 'Hedonism', \n",
    "    'Self-Direction'\n",
    "]\n",
    "\n",
    "# Check if all required columns are present in the dataframe\n",
    "missing_columns = [col for col in required_columns if col not in scores_df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    # Select only the required columns\n",
    "    svs_df = scores_df[required_columns]\n",
    "\n",
    "    # svs_scores_df.to_csv('svs_scores_filtered.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqar\\AppData\\Local\\Temp\\ipykernel_15292\\368897620.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  svs_df['patient_id'] = svs_df['patient_id'].str.replace(',', '')\n",
      "C:\\Users\\Waqar\\AppData\\Local\\Temp\\ipykernel_15292\\368897620.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  svs_df['patient_id'] = pd.to_numeric(svs_df['patient_id'], errors='coerce').astype('Int64')\n"
     ]
    }
   ],
   "source": [
    "# Filter out messages from the patient only\n",
    "patient_transcripts = transcript_df[transcript_df['Role'] == 'Patient']\n",
    "\n",
    "# Aggregate patient messages by conversation ID\n",
    "# For simplicity, we'll concatenate all patient messages for each conversation\n",
    "patient_conversations = patient_transcripts.groupby('ID')['Message'].apply(lambda msgs: ' '.join(msgs)).reset_index()\n",
    "\n",
    "\n",
    "# marking the fields as int\n",
    "patient_conversations['ID'] = pd.to_numeric(patient_conversations['ID'], errors='coerce').astype('Int64')\n",
    "svs_df['patient_id'] = svs_df['patient_id'].str.replace(',', '')\n",
    "svs_df['patient_id'] = pd.to_numeric(svs_df['patient_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Assuming 'data' is your DataFrame containing the message and target variables\n",
    "data = pd.merge(patient_conversations, svs_df, left_on='ID', right_on='patient_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and target variable\n",
    "X = data['Message']\n",
    "y = data[required_columns].drop(columns='patient_id')\n",
    "\n",
    "# Handle NaN values\n",
    "y = y.dropna()\n",
    "X = X[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Conservation: 3.7556877901447403\n",
      "Mean Squared Error for Conformity: 6.302463278568738\n",
      "Mean Squared Error for Tradition: 3.766440409261476\n",
      "Mean Squared Error for Security: 5.573490662854732\n",
      "Mean Squared Error for Self-Transcendance: 2.5189650809495023\n",
      "Mean Squared Error for Benevolence: 1.6787592863777936\n",
      "Mean Squared Error for Universalism: 5.195116175895041\n",
      "Mean Squared Error for Self-Enhancement: 3.7083087444657\n",
      "Mean Squared Error for Power: 4.89742139947939\n",
      "Mean Squared Error for Achievement: 4.459321763457823\n",
      "Mean Squared Error for Stimulation: 5.822897844316516\n",
      "Mean Squared Error for Openness to Change: 2.533934361992602\n",
      "Mean Squared Error for Hedonism: 4.547830033094942\n",
      "Mean Squared Error for Self-Direction: 3.756290026578057\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a separate model for each SVS score\n",
    "models = {}\n",
    "predictions = pd.DataFrame(index=y_test.index)\n",
    "\n",
    "for column in y.columns:\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train[column])\n",
    "    models[column] = model\n",
    "    predictions[column] = model.predict(X_test)\n",
    "\n",
    "for column in y.columns:\n",
    "    # Drop NaN values from both y_test and predictions for the current column\n",
    "    valid_indices = y_test[column].dropna().index.intersection(predictions[column].index)\n",
    "    \n",
    "    # Align y_test and predictions to valid indices\n",
    "    y_test_valid = y_test.loc[valid_indices, column]\n",
    "    predictions_valid = predictions.loc[valid_indices, column]\n",
    "    \n",
    "    # Check if valid indices result in non-empty data\n",
    "    if len(y_test_valid) == 0 or len(predictions_valid) == 0:\n",
    "        print(f'No valid samples for {column}. Skipping MSE calculation.')\n",
    "        continue\n",
    "    \n",
    "    # Calculate and print the Mean Squared Error\n",
    "    mse = mean_squared_error(y_test_valid, predictions_valid)\n",
    "    print(f'Mean Squared Error for {column}: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 215ms/step - loss: 37.3919 - val_loss: 35.4118\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 32.2962 - val_loss: 26.3889\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 22.4223 - val_loss: 12.4483\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 9.8331 - val_loss: 5.9880\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 6.6199 - val_loss: 10.8108\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 10.7453 - val_loss: 5.7267\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 3.9056 - val_loss: 6.0129\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 3.5704 - val_loss: 7.6827\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 4.5633 - val_loss: 7.1471\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - loss: 3.9612 - val_loss: 5.6805\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.8897 - val_loss: 5.0369\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.7831 - val_loss: 5.1475\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - loss: 2.4688 - val_loss: 4.8061\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 1.4045 - val_loss: 5.3287\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 1.2357 - val_loss: 6.2603\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 1.7831 - val_loss: 6.0688\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 1.3854 - val_loss: 5.1085\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.9739 - val_loss: 4.5319\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.1609 - val_loss: 4.5053\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.8204 - val_loss: 4.8278\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.6933 - val_loss: 5.1104\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 0.9220 - val_loss: 4.9639\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - loss: 0.7445 - val_loss: 4.6602\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7212 - val_loss: 4.5730\n",
      "Epoch 25/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.7173 - val_loss: 4.6954\n",
      "Epoch 26/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.5703 - val_loss: 5.0098\n",
      "Epoch 27/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.6597 - val_loss: 5.0963\n",
      "Epoch 28/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.5578 - val_loss: 4.9184\n",
      "Epoch 29/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.4046 - val_loss: 4.7692\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Mean Squared Error for Conservation: 3.802365779876709\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 216ms/step - loss: 36.7011 - val_loss: 33.1638\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 29.1762 - val_loss: 22.9374\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 20.2351 - val_loss: 9.7517\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 7.8372 - val_loss: 9.7539\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 10.5237 - val_loss: 9.5153\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 8.5529 - val_loss: 6.5463\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 4.2726 - val_loss: 8.0101\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 4.3653 - val_loss: 8.4775\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 4.4792 - val_loss: 7.1995\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.3655 - val_loss: 6.3033\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2.0148 - val_loss: 6.2733\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.5687 - val_loss: 6.2371\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 1.9107 - val_loss: 6.5811\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.9582 - val_loss: 7.8680\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.7108 - val_loss: 8.1981\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 1.3912 - val_loss: 7.3215\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - loss: 1.1709 - val_loss: 6.4665\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.8493 - val_loss: 6.3291\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.2829 - val_loss: 6.5582\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.5834 - val_loss: 7.5085\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 0.7799 - val_loss: 8.0610\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - loss: 0.7918 - val_loss: 7.4764\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Mean Squared Error for Conformity: 6.5960774421691895\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - loss: 37.3218 - val_loss: 35.4293\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 32.0660 - val_loss: 26.9086\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 21.5288 - val_loss: 13.4847\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 9.7926 - val_loss: 7.1868\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 7.2993 - val_loss: 10.8456\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 10.2942 - val_loss: 7.2066\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 4.7766 - val_loss: 8.2371\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 4.1438 - val_loss: 9.8802\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 5.5151 - val_loss: 9.0793\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 4.0580 - val_loss: 7.6528\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.5550 - val_loss: 7.3642\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 2.6342 - val_loss: 7.4660\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.8076 - val_loss: 7.6116\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - loss: 1.4072 - val_loss: 8.1389\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Mean Squared Error for Tradition: 6.6060709953308105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 313ms/step - loss: 45.4464 - val_loss: 40.2914\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 38.9071 - val_loss: 31.6291\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 29.2976 - val_loss: 17.2605\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 14.4236 - val_loss: 5.8808\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 7.5724 - val_loss: 11.4566\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 10.1989 - val_loss: 6.7563\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 5.1362 - val_loss: 5.6363\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 3.7530 - val_loss: 7.2246\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 418ms/step - loss: 5.1005 - val_loss: 7.2870\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - loss: 3.8195 - val_loss: 5.6218\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - loss: 2.4511 - val_loss: 4.5983\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.8819 - val_loss: 4.5697\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 2.3421 - val_loss: 4.3547\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - loss: 1.2644 - val_loss: 4.3947\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.4636 - val_loss: 4.9523\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.9478 - val_loss: 5.1463\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1.3244 - val_loss: 4.7572\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1.1765 - val_loss: 4.3124\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.9700 - val_loss: 4.2164\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - loss: 0.7394 - val_loss: 4.2972\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.6145 - val_loss: 4.3540\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.9673 - val_loss: 4.3978\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 0.7203 - val_loss: 4.4250\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 0.7311 - val_loss: 4.6835\n",
      "Epoch 25/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.6866 - val_loss: 4.6424\n",
      "Epoch 26/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 0.7249 - val_loss: 4.4446\n",
      "Epoch 27/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.9189 - val_loss: 4.3961\n",
      "Epoch 28/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.1419 - val_loss: 4.7538\n",
      "Epoch 29/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.6445 - val_loss: 5.1150\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "Mean Squared Error for Security: 4.723831653594971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 241ms/step - loss: 45.2157 - val_loss: 44.6085\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 39.6082 - val_loss: 34.3174\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 28.7655 - val_loss: 17.0495\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 13.2556 - val_loss: 3.1225\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 5.6643 - val_loss: 7.9027\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 10.5791 - val_loss: 3.3226\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 3.3864 - val_loss: 4.4023\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 3.9464 - val_loss: 7.1697\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 5.3188 - val_loss: 6.8179\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 3.6522 - val_loss: 4.5065\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - loss: 2.4688 - val_loss: 2.7485\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 1.9218 - val_loss: 3.0583\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 3.8756 - val_loss: 2.6873\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 1.6301 - val_loss: 2.9290\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 1.1608 - val_loss: 4.0163\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 1.2519 - val_loss: 4.3639\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - loss: 1.3947 - val_loss: 3.6562\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 1.0217 - val_loss: 2.8045\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 1.0164 - val_loss: 2.5680\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233ms/step - loss: 0.8635 - val_loss: 2.6875\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.8888 - val_loss: 3.3600\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.8121 - val_loss: 3.6463\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - loss: 1.0065 - val_loss: 3.5360\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - loss: 0.9133 - val_loss: 3.2428\n",
      "Epoch 25/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.7438 - val_loss: 2.8001\n",
      "Epoch 26/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - loss: 0.6706 - val_loss: 2.6726\n",
      "Epoch 27/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - loss: 1.1067 - val_loss: 2.8881\n",
      "Epoch 28/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.8760 - val_loss: 3.5501\n",
      "Epoch 29/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 1.0727 - val_loss: 3.8359\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Mean Squared Error for Self-Transcendance: 3.6471478939056396\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 508ms/step - loss: 51.9028 - val_loss: 51.2657\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 46.6646 - val_loss: 41.4614\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - loss: 37.0614 - val_loss: 24.4020\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 19.1696 - val_loss: 5.8711\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 5.1090 - val_loss: 8.7163\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 187ms/step - loss: 11.3705 - val_loss: 6.1401\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 7.3481 - val_loss: 4.1609\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 3.6779 - val_loss: 7.4610\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 6.0329 - val_loss: 8.5638\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 5.8602 - val_loss: 6.5833\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step - loss: 3.8704 - val_loss: 3.9977\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 2.2893 - val_loss: 3.6180\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 2.0585 - val_loss: 3.7214\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 2.7106 - val_loss: 3.5456\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 1.9897 - val_loss: 4.4221\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.6135 - val_loss: 5.1601\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 2.1869 - val_loss: 4.5140\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 1.5280 - val_loss: 3.5046\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 1.3716 - val_loss: 3.2072\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 1.1664 - val_loss: 3.2164\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 1.1991 - val_loss: 3.5344\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 1.0407 - val_loss: 4.1240\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 1.1357 - val_loss: 4.2451\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 1.0556 - val_loss: 4.1126\n",
      "Epoch 25/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.9101 - val_loss: 3.8017\n",
      "Epoch 26/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.7131 - val_loss: 3.7222\n",
      "Epoch 27/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.8389 - val_loss: 3.7884\n",
      "Epoch 28/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.8645 - val_loss: 3.8371\n",
      "Epoch 29/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.7906 - val_loss: 3.7586\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "Mean Squared Error for Benevolence: 3.6246836185455322\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 40.8321 - val_loss: 41.8860\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 39.2279 - val_loss: 36.7774\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 32.5102 - val_loss: 27.1790\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 23.6102 - val_loss: 13.1819\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 10.5893 - val_loss: 3.4752\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 6.4747 - val_loss: 6.3236\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 7.7718 - val_loss: 3.9823\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 6.5652 - val_loss: 3.7684\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 3.6462 - val_loss: 5.3903\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 3.5717 - val_loss: 5.7234\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 3.7493 - val_loss: 4.7211\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 3.1015 - val_loss: 3.4774\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 2.2995 - val_loss: 3.1454\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 1.6293 - val_loss: 3.1316\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 2.2302 - val_loss: 3.2830\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1.6662 - val_loss: 3.6982\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.2293 - val_loss: 3.8420\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.3236 - val_loss: 3.6029\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.0529 - val_loss: 3.3983\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.9671 - val_loss: 3.4411\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7371 - val_loss: 3.5800\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 0.8272 - val_loss: 3.8975\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.8407 - val_loss: 3.8908\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6782 - val_loss: 3.6133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Mean Squared Error for Universalism: 5.212072372436523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 193ms/step - loss: 25.3107 - val_loss: 20.0621\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 22.8266 - val_loss: 15.0537\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 16.7955 - val_loss: 7.5527\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 7.9718 - val_loss: 4.0828\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 7.1622 - val_loss: 7.1323\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 8.4256 - val_loss: 4.2248\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 4.6038 - val_loss: 3.5378\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 3.1232 - val_loss: 3.9078\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 2.8997 - val_loss: 3.8223\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 3.4372 - val_loss: 3.3576\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 2.2508 - val_loss: 3.1959\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 1.4810 - val_loss: 3.1918\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 1.2833 - val_loss: 3.0947\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.0331 - val_loss: 3.1207\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.0181 - val_loss: 3.1831\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9488 - val_loss: 3.1918\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6874 - val_loss: 3.1511\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 1.0987 - val_loss: 3.1390\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7621 - val_loss: 3.1269\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.6854 - val_loss: 3.0714\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.6602 - val_loss: 3.0317\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.5727 - val_loss: 3.1044\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.4806 - val_loss: 3.1177\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.5442 - val_loss: 3.0816\n",
      "Epoch 25/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.6357 - val_loss: 2.9888\n",
      "Epoch 26/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.6720 - val_loss: 3.0032\n",
      "Epoch 27/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7787 - val_loss: 3.0789\n",
      "Epoch 28/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.6438 - val_loss: 3.1599\n",
      "Epoch 29/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.5102 - val_loss: 3.1156\n",
      "Epoch 30/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.5646 - val_loss: 2.9574\n",
      "Epoch 31/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - loss: 0.4773 - val_loss: 2.9097\n",
      "Epoch 32/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.6110 - val_loss: 2.9673\n",
      "Epoch 33/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.5782 - val_loss: 3.1706\n",
      "Epoch 34/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.5800 - val_loss: 3.2473\n",
      "Epoch 35/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.4301 - val_loss: 3.0023\n",
      "Epoch 36/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7892 - val_loss: 2.9184\n",
      "Epoch 37/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.6326 - val_loss: 2.9216\n",
      "Epoch 38/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8049 - val_loss: 3.1906\n",
      "Epoch 39/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.5428 - val_loss: 3.7964\n",
      "Epoch 40/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.5651 - val_loss: 3.3756\n",
      "Epoch 41/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.5925 - val_loss: 2.9693\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Mean Squared Error for Self-Enhancement: 4.379092216491699\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - loss: 18.3651 - val_loss: 12.4571\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 15.3705 - val_loss: 9.4927\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 12.1896 - val_loss: 6.1499\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 7.2764 - val_loss: 7.7704\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 6.8834 - val_loss: 8.8964\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 7.5098 - val_loss: 6.3240\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 4.2718 - val_loss: 5.3317\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 4.1086 - val_loss: 5.2589\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - loss: 3.8845 - val_loss: 5.1118\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 3.2192 - val_loss: 5.1126\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 2.6488 - val_loss: 5.4862\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 2.2274 - val_loss: 5.5538\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 2.0716 - val_loss: 5.0835\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - loss: 1.1695 - val_loss: 4.8941\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.2605 - val_loss: 4.9568\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1.6826 - val_loss: 4.9740\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1.3553 - val_loss: 4.9204\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 1.0237 - val_loss: 4.9790\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 1.1886 - val_loss: 4.9890\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.6512 - val_loss: 5.0258\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 0.9909 - val_loss: 5.0402\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.6448 - val_loss: 4.9987\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.0065 - val_loss: 5.0073\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.7617 - val_loss: 4.9950\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Mean Squared Error for Power: 5.303617000579834\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 215ms/step - loss: 38.4159 - val_loss: 36.9286\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 32.5790 - val_loss: 29.6669\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 26.0727 - val_loss: 17.6274\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 13.4692 - val_loss: 7.0349\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 7.3447 - val_loss: 8.7660\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 9.8345 - val_loss: 6.9537\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 6.4501 - val_loss: 7.3394\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 4.9426 - val_loss: 8.9705\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 3.7880 - val_loss: 8.8569\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 3.6797 - val_loss: 7.4908\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 2.9364 - val_loss: 6.8071\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 2.2112 - val_loss: 6.8741\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.9079 - val_loss: 7.1162\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.5108 - val_loss: 8.0610\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.2319 - val_loss: 8.4108\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 1.2476 - val_loss: 7.7681\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 1.0412 - val_loss: 7.1231\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 1.2894 - val_loss: 7.1258\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 1.0578 - val_loss: 7.5470\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.7886 - val_loss: 7.8343\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.8810 - val_loss: 7.5308\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Mean Squared Error for Achievement: 6.161473751068115\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 184ms/step - loss: 25.4705 - val_loss: 27.6059\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 25.6362 - val_loss: 23.0121\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 20.2811 - val_loss: 15.2793\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 12.6282 - val_loss: 7.0867\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 6.6371 - val_loss: 7.1869\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 10.9306 - val_loss: 6.4401\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 8.1141 - val_loss: 5.4821\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 4.8746 - val_loss: 7.5708\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.4048 - val_loss: 8.5849\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 5.5114 - val_loss: 7.6566\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - loss: 4.5143 - val_loss: 5.7776\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2.5546 - val_loss: 4.4012\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 2.6586 - val_loss: 4.0014\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 2.8405 - val_loss: 3.9386\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 1.7878 - val_loss: 4.4102\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.2589 - val_loss: 5.2012\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 1.2432 - val_loss: 5.2129\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 1.1447 - val_loss: 4.6505\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8340 - val_loss: 4.3106\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9195 - val_loss: 4.1656\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7047 - val_loss: 4.1045\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.0465 - val_loss: 4.4718\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.5792 - val_loss: 4.9200\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.6224 - val_loss: 5.1001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Mean Squared Error for Stimulation: 6.206841468811035\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 181ms/step - loss: 32.9526 - val_loss: 29.0185\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 28.3577 - val_loss: 20.4077\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 17.7925 - val_loss: 8.5126\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 7.1272 - val_loss: 5.9784\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 7.7609 - val_loss: 7.5904\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 9.2741 - val_loss: 4.4292\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 3.5614 - val_loss: 5.3924\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 3.5893 - val_loss: 5.9892\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 3.7725 - val_loss: 5.1126\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 2.4033 - val_loss: 4.0915\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 1.8235 - val_loss: 3.7034\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.3707 - val_loss: 3.5718\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1.5244 - val_loss: 3.4748\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1566 - val_loss: 3.9835\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 1.0932 - val_loss: 4.3938\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.3023 - val_loss: 3.8755\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8366 - val_loss: 3.1471\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.0744 - val_loss: 3.0308\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8250 - val_loss: 3.2387\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.6343 - val_loss: 3.5608\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7339 - val_loss: 3.7143\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7703 - val_loss: 3.4052\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.7760 - val_loss: 3.2190\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.6067 - val_loss: 3.3350\n",
      "Epoch 25/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.7762 - val_loss: 3.4242\n",
      "Epoch 26/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.6165 - val_loss: 3.4726\n",
      "Epoch 27/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.4398 - val_loss: 3.4947\n",
      "Epoch 28/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.6627 - val_loss: 3.6553\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Mean Squared Error for Openness to Change: 3.3957674503326416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 254ms/step - loss: 33.8830 - val_loss: 22.7685\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 30.1876 - val_loss: 18.2898\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 24.3563 - val_loss: 11.0015\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 15.4567 - val_loss: 5.7248\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 8.1257 - val_loss: 10.7468\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 9.6650 - val_loss: 9.7148\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 7.8358 - val_loss: 6.2949\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 4.9633 - val_loss: 5.5194\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 4.8512 - val_loss: 5.5371\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 4.2809 - val_loss: 5.4959\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 2.9160 - val_loss: 5.6228\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 2.3800 - val_loss: 5.8989\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 1.9958 - val_loss: 5.9216\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.3375 - val_loss: 5.6936\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 1.2685 - val_loss: 5.4413\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 1.3275 - val_loss: 5.4419\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 1.4195 - val_loss: 5.3853\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.9888 - val_loss: 5.3334\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 216ms/step - loss: 1.3154 - val_loss: 5.3410\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 1.1641 - val_loss: 5.3150\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 1.1017 - val_loss: 5.3624\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 1.3622 - val_loss: 5.3307\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 1.0751 - val_loss: 5.3816\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.7126 - val_loss: 5.5112\n",
      "Epoch 25/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - loss: 0.7573 - val_loss: 5.6104\n",
      "Epoch 26/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 0.8121 - val_loss: 5.6352\n",
      "Epoch 27/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.6619 - val_loss: 5.6437\n",
      "Epoch 28/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 1.1189 - val_loss: 5.6086\n",
      "Epoch 29/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.8054 - val_loss: 5.5960\n",
      "Epoch 30/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.9087 - val_loss: 5.6638\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Mean Squared Error for Hedonism: 5.790571212768555\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 323ms/step - loss: 44.8972 - val_loss: 48.7962\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 40.7213 - val_loss: 39.5253\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 191ms/step - loss: 28.8546 - val_loss: 23.8005\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - loss: 14.4718 - val_loss: 8.3385\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - loss: 5.2716 - val_loss: 12.5730\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 12.9338 - val_loss: 8.9285\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 6.1562 - val_loss: 7.5843\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 3.0801 - val_loss: 10.6938\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 4.8776 - val_loss: 11.8818\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 5.5277 - val_loss: 10.1751\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 3.6823 - val_loss: 7.7433\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 2.4209 - val_loss: 6.6609\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 1.8623 - val_loss: 6.5762\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 2.1315 - val_loss: 6.7612\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 1.5666 - val_loss: 7.4562\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 1.2022 - val_loss: 8.1920\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1.3044 - val_loss: 8.1866\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - loss: 1.1680 - val_loss: 7.4430\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 1.0800 - val_loss: 6.9257\n",
      "Epoch 20/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206ms/step - loss: 0.7061 - val_loss: 6.7379\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - loss: 0.8561 - val_loss: 6.8526\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.9506 - val_loss: 6.9230\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.6848 - val_loss: 7.0281\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Mean Squared Error for Self-Direction: 4.544477462768555\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning Keras Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Convert text messages to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Convert target variables to float\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert sparse matrix to dense format\n",
    "X_train_dense = X_train.toarray().astype(np.float32)\n",
    "X_test_dense = X_test.toarray().astype(np.float32)\n",
    "\n",
    "# Build and compile the neural network model\n",
    "models = {}\n",
    "predictions = pd.DataFrame(index=y_test.index)\n",
    "\n",
    "for column in y.columns:\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(X_train_dense.shape[1],)),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1)  # Output layer for regression, one neuron per column\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Set up early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_dense,\n",
    "        y_train[column],\n",
    "        validation_split=0.2,\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Predict on the test set\n",
    "    predictions[column] = model.predict(X_test_dense).flatten()\n",
    "\n",
    "    # Calculate and print Mean Squared Error\n",
    "    mse = mean_squared_error(y_test[column], predictions[column])\n",
    "    print(f'Mean Squared Error for {column}: {mse}')\n",
    "    \n",
    "    # Optionally, plot training history\n",
    "    # import matplotlib.pyplot as plt\n",
    "\n",
    "    # plt.plot(history.history['loss'], label='Training Loss')\n",
    "    # plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.title(f'Training History for {column}')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example for one SVS score, e.g., 'Benevolence'\n",
    "score = 'Benevolence'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_valid, predictions_valid, alpha=0.5)\n",
    "plt.plot([min(y_test_valid), max(y_test_valid)], [min(y_test_valid), max(y_test_valid)], color='red', linestyle='--')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Actual vs Predicted Values for {score}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\projects\\personality-trait\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                               \n",
      " 33%|███▎      | 12/36 [01:54<02:56,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': -2.447676420211792, 'eval_runtime': 10.1511, 'eval_samples_per_second': 2.266, 'eval_steps_per_second': 0.296, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|██████▋   | 24/36 [04:18<01:59,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': -5.031106948852539, 'eval_runtime': 10.0185, 'eval_samples_per_second': 2.296, 'eval_steps_per_second': 0.299, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 36/36 [07:02<00:00, 11.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': -5.7651166915893555, 'eval_runtime': 11.8526, 'eval_samples_per_second': 1.94, 'eval_steps_per_second': 0.253, 'epoch': 3.0}\n",
      "{'train_runtime': 422.3139, 'train_samples_per_second': 0.632, 'train_steps_per_second': 0.085, 'train_loss': -3.0619994269476996, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Conservation: 18.69681167602539\n",
      "Mean Squared Error for Conformity: 21.470775604248047\n",
      "Mean Squared Error for Tradition: 21.133760452270508\n",
      "Mean Squared Error for Security: 23.72403335571289\n",
      "Mean Squared Error for Self-Transcendance: 28.626449584960938\n",
      "Mean Squared Error for Benevolence: 37.971622467041016\n",
      "Mean Squared Error for Universalism: 29.052845001220703\n",
      "Mean Squared Error for Self-Enhancement: 14.754504203796387\n",
      "Mean Squared Error for Power: 12.910859107971191\n",
      "Mean Squared Error for Achievement: 25.12451171875\n",
      "Mean Squared Error for Stimulation: 21.18195343017578\n",
      "Mean Squared Error for Openness to Change: 18.74834632873535\n",
      "Mean Squared Error for Hedonism: 18.279821395874023\n",
      "Mean Squared Error for Self-Direction: 28.103839874267578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Convert target variables to float\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = torch.tensor(self.labels.iloc[idx].values).float()\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "# Load a pre-trained tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=y.shape[1])\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer, max_len=128)\n",
    "test_dataset = TextDataset(X_test, y_test, tokenizer, max_len=128)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predictions_logits = predictions.predictions\n",
    "\n",
    "# Calculate Mean Squared Error for each column\n",
    "for i, column in enumerate(y.columns):\n",
    "    mse = mean_squared_error(y_test[column], predictions_logits[:, i])\n",
    "    print(f'Mean Squared Error for {column}: {mse}')\n",
    "\n",
    "# Optionally, save the model and tokenizer\n",
    "# model.save_pretrained('./saved_model')\n",
    "# tokenizer.save_pretrained('./saved_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
